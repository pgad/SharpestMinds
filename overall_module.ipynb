{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Fashion MNIST\n",
    "### Implementing CNN models to classify Fashion MNIST and compare their performances\n",
    "##### Download data from kaggle.com/zalando-research/fashionmnist/version/4\n",
    "##### Unzip, put .csv files into the same folder as the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import useful modules here\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are these many examples in test set: 10000\n",
      "There are these many examples in train set: 48000\n",
      "There are these many examples in validation set: 12000\n",
      "There are these many pixels per example: 784\n"
     ]
    }
   ],
   "source": [
    "#load test and train data\n",
    "\n",
    "from load_data import load_csv_as_array\n",
    "test_data = load_csv_as_array(\"fashion-mnist_test.csv\")\n",
    "train_data = load_csv_as_array(\"fashion-mnist_train.csv\")\n",
    "\n",
    "#The 0th column contains the label, 0th row contains column descriptions\n",
    "#Discard the 0th row\n",
    "#Separate the data along the 0th column\n",
    "\n",
    "#labels\n",
    "\n",
    "testY = test_data[:,0]\n",
    "\n",
    "Y = train_data[:,0]\n",
    "\n",
    "#data\n",
    "#Scale the data between 0-1\n",
    "\n",
    "testX = test_data[:,1:]/255\n",
    "\n",
    "X = train_data[:,1:]/255\n",
    "\n",
    "#Get the train and validation set from X (80/20 split)\n",
    "\n",
    "trainX, validX, trainY, validY = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"There are these many examples in test set:\", testY.shape[0])\n",
    "print(\"There are these many examples in train set:\", trainY.shape[0])\n",
    "print(\"There are these many examples in validation set:\", validY.shape[0])\n",
    "print(\"There are these many pixels per example:\", testX.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert the row-wise features (784 pixels) into a 2-D 28x28 image\n",
    "trainX = trainX.reshape((trainX.shape[0], 28,28,1))\n",
    "testX = testX.reshape((testX.shape[0], 28,28,1))\n",
    "validX = validX.reshape((validX.shape[0], 28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline CNN\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#construct the CNN model\n",
    "simpCNNModel = Sequential()\n",
    "simpCNNModel.add(Conv2D(28, (3, 3), strides=(2,2), activation='relu',\n",
    "            input_shape = (28,28,1)))\n",
    "simpCNNModel.add(Conv2D(14, (3, 3), strides=(2,2), activation='relu'))\n",
    "simpCNNModel.add(Flatten())\n",
    "simpCNNModel.add(Dense(128, activation='softplus', kernel_regularizer=l2(0)))\n",
    "simpCNNModel.add(Dense(32, activation='softplus'))\n",
    "simpCNNModel.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "\n",
    "optimizer = Adam(lr = .0001, decay = 5e-5)\n",
    "\n",
    "simpCNNModel.compile(optimizer=optimizer,\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 28)        280       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 6, 14)          3542      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 504)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               64640     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 72,920\n",
      "Trainable params: 72,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/15\n",
      "48000/48000 [==============================] - 14s - loss: 2.5165 - acc: 0.1074 - val_loss: 2.1522 - val_acc: 0.1982\n",
      "Epoch 2/15\n",
      "48000/48000 [==============================] - 10s - loss: 1.6162 - acc: 0.4891 - val_loss: 1.0259 - val_acc: 0.6533\n",
      "Epoch 3/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.8502 - acc: 0.7050 - val_loss: 0.7477 - val_acc: 0.7358\n",
      "Epoch 4/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.7119 - acc: 0.7482 - val_loss: 0.6752 - val_acc: 0.7563\n",
      "Epoch 5/15\n",
      "48000/48000 [==============================] - 9s - loss: 0.6576 - acc: 0.7647 - val_loss: 0.6381 - val_acc: 0.7645\n",
      "Epoch 6/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.6240 - acc: 0.7739 - val_loss: 0.6054 - val_acc: 0.7769\n",
      "Epoch 7/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5990 - acc: 0.7812 - val_loss: 0.5937 - val_acc: 0.7813\n",
      "Epoch 8/15\n",
      "48000/48000 [==============================] - 12s - loss: 0.5826 - acc: 0.7858 - val_loss: 0.5705 - val_acc: 0.7898\n",
      "Epoch 9/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5669 - acc: 0.7934 - val_loss: 0.5642 - val_acc: 0.7910\n",
      "Epoch 10/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5531 - acc: 0.7993 - val_loss: 0.5458 - val_acc: 0.8031\n",
      "Epoch 11/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5418 - acc: 0.8045 - val_loss: 0.5385 - val_acc: 0.8032\n",
      "Epoch 12/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5297 - acc: 0.8091 - val_loss: 0.5235 - val_acc: 0.8120\n",
      "Epoch 13/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5212 - acc: 0.8123 - val_loss: 0.5168 - val_acc: 0.8117\n",
      "Epoch 14/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5153 - acc: 0.8158 - val_loss: 0.5068 - val_acc: 0.8187\n",
      "Epoch 15/15\n",
      "48000/48000 [==============================] - 11s - loss: 0.5075 - acc: 0.8191 - val_loss: 0.5057 - val_acc: 0.8174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279a1cc5e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit and evaluate\n",
    "print(simpCNNModel.summary())\n",
    "\n",
    "simpCNNModel.fit(trainX, trainY,\n",
    "            batch_size=512, epochs=15, verbose = 1,\n",
    "            validation_data=(validX, validY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/15\n",
      "48000/48000 [==============================] - 14s - loss: 2.5718 - acc: 0.0991 - val_loss: 2.3286 - val_acc: 0.1036\n",
      "Epoch 2/15\n",
      "48000/48000 [==============================] - 10s - loss: 2.2025 - acc: 0.2824 - val_loss: 2.0505 - val_acc: 0.3833\n",
      "Epoch 3/15\n",
      "48000/48000 [==============================] - 9s - loss: 1.7169 - acc: 0.4446 - val_loss: 1.3246 - val_acc: 0.5578\n",
      "Epoch 4/15\n",
      "48000/48000 [==============================] - 8s - loss: 1.0927 - acc: 0.6200 - val_loss: 0.9395 - val_acc: 0.6752\n",
      "Epoch 5/15\n",
      "48000/48000 [==============================] - 9s - loss: 0.8614 - acc: 0.7024 - val_loss: 0.8016 - val_acc: 0.7169\n",
      "Epoch 6/15\n",
      "48000/48000 [==============================] - 8s - loss: 0.7641 - acc: 0.7285 - val_loss: 0.7339 - val_acc: 0.7351\n",
      "Epoch 7/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.7094 - acc: 0.7440 - val_loss: 0.6911 - val_acc: 0.7480\n",
      "Epoch 8/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.6717 - acc: 0.7563 - val_loss: 0.6631 - val_acc: 0.7562\n",
      "Epoch 9/15\n",
      "48000/48000 [==============================] - 11s - loss: 0.6444 - acc: 0.7659 - val_loss: 0.6357 - val_acc: 0.7658\n",
      "Epoch 10/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.6229 - acc: 0.7725 - val_loss: 0.6152 - val_acc: 0.7766\n",
      "Epoch 11/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.6053 - acc: 0.7786 - val_loss: 0.6018 - val_acc: 0.7778\n",
      "Epoch 12/15\n",
      "48000/48000 [==============================] - 10s - loss: 0.5909 - acc: 0.7836 - val_loss: 0.5839 - val_acc: 0.7860\n",
      "Epoch 13/15\n",
      "48000/48000 [==============================] - 9s - loss: 0.5773 - acc: 0.7898 - val_loss: 0.5758 - val_acc: 0.7868\n",
      "Epoch 14/15\n",
      "48000/48000 [==============================] - 9s - loss: 0.5671 - acc: 0.7939 - val_loss: 0.5656 - val_acc: 0.7930\n",
      "Epoch 15/15\n",
      "48000/48000 [==============================] - 9s - loss: 0.5580 - acc: 0.7968 - val_loss: 0.5599 - val_acc: 0.7904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279a725d128>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try to improve on the baseline\n",
    "#Let's try adding a maxpool layer\n",
    "\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "\n",
    "#construct the CNN model\n",
    "CNNModel = Sequential()\n",
    "CNNModel.add(Conv2D(28, (3, 3), strides=(2,2), activation='relu',\n",
    "            input_shape = (28,28,1)))\n",
    "CNNModel.add(Conv2D(14, (3, 3), strides=(2,2), activation='relu'))\n",
    "CNNModel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "CNNModel.add(Flatten())\n",
    "CNNModel.add(Dense(128, activation='softplus', kernel_regularizer=l2(0)))\n",
    "CNNModel.add(Dense(32, activation='softplus'))\n",
    "CNNModel.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "\n",
    "optimizer = Adam(lr = .0001, decay = 5e-5)\n",
    "\n",
    "CNNModel.compile(optimizer=optimizer,\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#fit and evaluate\n",
    "CNNModel.fit(trainX, trainY,\n",
    "            batch_size=512, epochs=15, verbose = 1,\n",
    "            validation_data=(validX, validY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/15\n",
      "48000/48000 [==============================] - 42s - loss: 13.8731 - acc: 0.2854 - val_loss: 13.0864 - val_acc: 0.1420\n",
      "Epoch 2/15\n",
      "48000/48000 [==============================] - 34s - loss: 11.1758 - acc: 0.5417 - val_loss: 11.1141 - val_acc: 0.2830\n",
      "Epoch 3/15\n",
      "48000/48000 [==============================] - 35s - loss: 9.1698 - acc: 0.6790 - val_loss: 9.3041 - val_acc: 0.3840\n",
      "Epoch 4/15\n",
      "48000/48000 [==============================] - 34s - loss: 7.5963 - acc: 0.7327 - val_loss: 7.6390 - val_acc: 0.5617\n",
      "Epoch 5/15\n",
      "48000/48000 [==============================] - 33s - loss: 6.3180 - acc: 0.7587 - val_loss: 6.1883 - val_acc: 0.6811\n",
      "Epoch 6/15\n",
      "48000/48000 [==============================] - 33s - loss: 5.2644 - acc: 0.7765 - val_loss: 5.0239 - val_acc: 0.7487\n",
      "Epoch 7/15\n",
      "48000/48000 [==============================] - 33s - loss: 4.3908 - acc: 0.7914 - val_loss: 4.1199 - val_acc: 0.7804\n",
      "Epoch 8/15\n",
      "48000/48000 [==============================] - 34s - loss: 3.6645 - acc: 0.8035 - val_loss: 3.4076 - val_acc: 0.7997\n",
      "Epoch 9/15\n",
      "48000/48000 [==============================] - 33s - loss: 3.0615 - acc: 0.8139 - val_loss: 2.8382 - val_acc: 0.8113\n",
      "Epoch 10/15\n",
      "48000/48000 [==============================] - 33s - loss: 2.5603 - acc: 0.8233 - val_loss: 2.3734 - val_acc: 0.8182\n",
      "Epoch 11/15\n",
      "48000/48000 [==============================] - 34s - loss: 2.1471 - acc: 0.8310 - val_loss: 1.9985 - val_acc: 0.8247\n",
      "Epoch 12/15\n",
      "48000/48000 [==============================] - 34s - loss: 1.8043 - acc: 0.8386 - val_loss: 1.6861 - val_acc: 0.8311\n",
      "Epoch 13/15\n",
      "48000/48000 [==============================] - 33s - loss: 1.5226 - acc: 0.8448 - val_loss: 1.4302 - val_acc: 0.8361\n",
      "Epoch 14/15\n",
      "48000/48000 [==============================] - 33s - loss: 1.2911 - acc: 0.8502 - val_loss: 1.2238 - val_acc: 0.8415\n",
      "Epoch 15/15\n",
      "48000/48000 [==============================] - 36s - loss: 1.1030 - acc: 0.8549 - val_loss: 1.0486 - val_acc: 0.8497\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x279a8108d68>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#That doesn't really change much (~4% isn't that much of a difference)\n",
    "#Let's try adding batch normalization \n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#construct the CNN model\n",
    "CNNModel = Sequential()\n",
    "CNNModel.add(Conv2D(28, (3, 3), strides=(2,2), activation='relu',\n",
    "            input_shape = (28,28,1)))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(Conv2D(14, (3, 3), strides=(2,2), activation='relu'))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "CNNModel.add(Flatten())\n",
    "#add regularization here\n",
    "CNNModel.add(Dense(128, activation='softplus', kernel_regularizer=l2(0.1)))\n",
    "CNNModel.add(BatchNormalization())\n",
    "CNNModel.add(Dense(32, activation='softplus'))\n",
    "CNNModel.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#compile the model\n",
    "\n",
    "optimizer = Adam(lr = .0001, decay = 5e-5)\n",
    "\n",
    "CNNModel.compile(optimizer=optimizer,\n",
    "loss='sparse_categorical_crossentropy',\n",
    "metrics=['accuracy'])\n",
    "\n",
    "#fit and evaluate\n",
    "CNNModel.fit(trainX, trainY,\n",
    "            batch_size=512, epochs=15, verbose = 1,\n",
    "            validation_data=(validX, validY)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9888/10000 [============================>.] - ETA: 0stest loss: 0.9875359104156494\n",
      "test acc: 0.8507\n"
     ]
    }
   ],
   "source": [
    "#The best performer is CNNModel on Validation data\n",
    "\n",
    "loss, accuracy = CNNModel.evaluate(testX, testY)\n",
    "\n",
    "print('test loss:', loss)\n",
    "print('test acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further directions could be to use the validation results to tune hyperparameters. The learning rate used and filter sizes could all be tuned as hyper-parameters. We could also use something like t-SNE instead to tune the hyperparameters, instead of accuracy."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
